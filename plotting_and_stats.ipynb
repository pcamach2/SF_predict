{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# create a function for performing the paired t-test \n",
    "#     and for calculating the effect size of the paired t-test\n",
    "#     and also returning the means of the two samples \n",
    "def paired_ttest(x,y,alternative='two-sided'):\n",
    "    x_arr = np.array(x)\n",
    "    y_arr = np.array(y)\n",
    "    # t-test from scipy.stats\n",
    "    t_statistic, p_value = ttest_rel(x_arr,y_arr,alternative=alternative)\n",
    "    # Calculate effect size (Cohen's d)\n",
    "    # calculate mean of differences between two arrays\n",
    "    mean_diff = np.mean(x_arr - y_arr)\n",
    "    mean_x = np.mean(x_arr)\n",
    "    mean_y = np.mean(y_arr)\n",
    "    pooled_std = np.sqrt((np.std(x_arr) ** 2 + np.std(y_arr) ** 2) / 2)\n",
    "    d = mean_diff / pooled_std\n",
    "    return t_statistic, p_value, d, mean_x, mean_y\n",
    "\n",
    "# create a function for performing the shapiro-wilk test for normality\n",
    "#     of differences between two paired samples and for creating a qq plot\n",
    "#     of the differences between the two paired samples\n",
    "def shapiro_wilk_qq(x,y,recon1,recon2,walk_length):\n",
    "    # shapiro-wilk test for normality\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(x-y)\n",
    "    # # make figure\n",
    "    # plt.figure(figsize=(8,6))\n",
    "    # # qq plot\n",
    "    # qq = stats.probplot(x-y, dist=\"norm\", plot=plt)\n",
    "    # # plot name\n",
    "    # # qq_plot_name = '/datain/dataset/plots/normality/qq_plot_differences'+str(x)+'_'+str(y)+'.png' # Uncomment for use in singularity\n",
    "    # qq_plot_name = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/plots/normality/qq_plot_differences'+str(recon1)+'_'+str(recon2)+'_walklength_'+str(walk_length)+'.png'\n",
    "    # # make matplotlib plot\n",
    "    # plt.title('QQ Plot for Differences between '+str(recon1)+' and '+str(recon2))\n",
    "    # plt.xlabel('Theoretical Quantiles')\n",
    "    # plt.ylabel('Ordered Values')\n",
    "    # plt.tight_layout()\n",
    "    # # save qq plot\n",
    "    # plt.savefig(qq_plot_name)\n",
    "    # plt.close()\n",
    "    return shapiro_stat, shapiro_p\n",
    "\n",
    "# create a function for performing the wilcoxon signed rank test with paired samples \n",
    "#     and calculating the effect size of the paired samples wilcoxon signed rank test\n",
    "#     and also returning the medians of the two samples\n",
    "def wilcoxon_test(x,y,alternative='two-sided'):\n",
    "    x_arr = np.array(x)\n",
    "    y_arr = np.array(y)\n",
    "    w_statistic, p_value = stats.wilcoxon(x_arr,y_arr,alternative=alternative,zero_method='pratt',mode='exact')\n",
    "    median_x = np.median(x_arr)\n",
    "    median_y = np.median(y_arr)\n",
    "    # Calculate effect size (r)\n",
    "    r = w_statistic / (len(x_arr) * (len(y_arr) + 1) / 2)\n",
    "    return w_statistic, p_value, r, median_x, median_y\n",
    "\n",
    "\n",
    "# create a function that reads a dataframe and returns a dataframe with the\n",
    "#     values of column i for which the row values in the 'Recon 1' column equal to recon1 \n",
    "#     and the values in the 'Recon 2' column equal to recon2\n",
    "def get_df_int(df,recon1,recon2,i):\n",
    "    if i is not None:\n",
    "        df_i = df[(df['Recon 1'][i] == recon1) & (df['Recon 2'][i] == recon2)]\n",
    "    else:\n",
    "        df_i = df[(df['Recon 1'] == recon1) & (df['Recon 2'] == recon2)]\n",
    "    return df_i\n",
    "\n",
    "# # create a function for calculating the mean, standard deviation, and median of the\n",
    "# #     values in each column of a dataframe - where the 'Recon 1' is equal to recon1\n",
    "# #     and 'Recon 2' is equal to recon2\n",
    "# def get_stats(df,recon1,recon2):\n",
    "#     df_i = df.loc[(df['Recon 1'] == recon1) & (df['Recon 2'] == recon2)]\n",
    "#     mean = df_i.mean(axis=0)\n",
    "#     std = df_i.std(axis=0)\n",
    "#     median = df_i.median(axis=0)\n",
    "#     return mean, std, median\n",
    "\n",
    "def print_stats(df):\n",
    "    print('Mean:')\n",
    "    print(df.mean(axis=0))\n",
    "    print('Standard Deviation:')\n",
    "    print(df.std(axis=0))\n",
    "    print('Median:')\n",
    "    print(df.median(axis=0))\n",
    "    print('Min:')\n",
    "    print(df.min(axis=0))\n",
    "    print('Max:')\n",
    "    print(df.max(axis=0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in files and perform statistical tests of independence for Pearson scores at each walk length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "verbose = False\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path).glob('*batch?.csv')\n",
    "for file in file_list:\n",
    "    if verbose is True:\n",
    "        print(file)\n",
    "    df = pd.read_csv(file)\n",
    "    # ,ave to a copy of df, then perform Fisher's r to z transformation on Pearson scores\n",
    "    df_z = df.copy()\n",
    "    for i in range(1, 6):\n",
    "        df_z[str(i)] = np.arctanh(df_z[str(i)])\n",
    "    # print(df_z)\n",
    "    if verbose is True:\n",
    "        print(file.stem)\n",
    "    # compare difference in score between Recon methods for each walk length column\n",
    "    for i in range(1, 6):\n",
    "        if verbose is True:\n",
    "            print('Walk Length: '+str(i))\n",
    "        # get unique values of Recon column\n",
    "        recon_list = df['Recon'].unique()\n",
    "        df_stats_all = pd.DataFrame()\n",
    "        df_stats = pd.DataFrame()\n",
    "        # loop through each unique value of Recon column\n",
    "        for recon in recon_list:\n",
    "            if verbose is True:\n",
    "                print(recon)\n",
    "            # combine statistics from each Recon method into dataframe\n",
    "            df_stats = pd.DataFrame({'Recon': recon_list, 'Mean': df[df['Recon'] == str(recon)][str(i)].mean(), 'Stdev': df[df['Recon'] == str(recon)][str(i)].std(\n",
    "            ), 'Median': df[df['Recon'] == str(recon)][str(i)].median(), 'IQR': df[df['Recon'] == str(recon)][str(i)].quantile(q=0.75)-df[df['Recon'] == str(recon)][str(i)].quantile(q=0.25)})\n",
    "            # combine df_stats from each Recon method into dataframe\n",
    "            df_stats_all = pd.concat([df_stats, df_stats_all], axis=0)\n",
    "        # print(df_stats_all)\n",
    "        # save dataframe to csv\n",
    "        df_stats_all.to_csv(main_path+'/stats/'+file.stem +\n",
    "                            '_walk_length_'+str(i)+'_stats.csv', index=False)\n",
    "        # calculate p-value for difference in score between Recon methods in recon_list, with comparisons between each pair of Recon methods, save all statistics and p-values to csv\n",
    "        df_z_ttest_rel_results = pd.DataFrame()\n",
    "        df_z_shapiro_wilk_results = pd.DataFrame()\n",
    "        df_z_wilcoxon_results = pd.DataFrame()\n",
    "        for recon in recon_list:\n",
    "            for recon2 in recon_list:\n",
    "                if recon != recon2:\n",
    "                    if verbose is True:\n",
    "                        print(recon)\n",
    "                        print(recon2)\n",
    "                    # calculate t-test on z-scores, get t-statistic and p-value and effect size, means\n",
    "                    ttest_rel_z_score = paired_ttest(df_z[df_z['Recon'] == str(\n",
    "                        recon)][str(i)], df_z[df_z['Recon'] == str(recon2)][str(i)], alternative='less')\n",
    "                    if verbose:\n",
    "                        print(ttest_rel_z_score[2])\n",
    "                    # calculate shapiro-wilk test on z-scores, get W-statistic and p-value\n",
    "                    # get array of df_z values for recon and recon2\n",
    "                    recon_1_z_scores = np.array(df_z[df_z['Recon'] == str(recon)][str(i)])\n",
    "                    recon_2_z_scores = np.array(df_z[df_z['Recon'] == str(recon2)][str(i)])\n",
    "                    # calculate differences between arrays of recon and recon2 z-scores\n",
    "                    differences = recon_1_z_scores - recon_2_z_scores\n",
    "                    shapiro_wilk_w, shapiro_wilk_p = stats.shapiro(differences)\n",
    "                    if verbose:\n",
    "                        print(shapiro_wilk_p)\n",
    "                    # calculate wilcoxon test on z-scores, get test statistic and p-value and effect size, medians\n",
    "                    wilcoxon_results = wilcoxon_test(df_z[df_z['Recon'] == str(\n",
    "                        recon)][str(i)], df_z[df_z['Recon'] == str(recon2)][str(i)], alternative='less')\n",
    "                    # combine stats into dataframe\n",
    "                    df_z_ttest_rel_results = pd.concat([df_z_ttest_rel_results, pd.DataFrame({'Recon 1': recon, 'Recon 2': recon2, 'T-statistic Paired T-test': ttest_rel_z_score[0],\n",
    "                                                                                                'p-value Paired T-test': ttest_rel_z_score[1], 'Effect Size d': ttest_rel_z_score[2],\n",
    "                                                                                                'Mean 1': ttest_rel_z_score[3], 'Mean 2': ttest_rel_z_score[4]}, index=[0])], axis=0)\n",
    "                    df_z_shapiro_wilk_results = pd.concat([df_z_shapiro_wilk_results, pd.DataFrame({'Recon 1': recon, 'Recon 2': recon2, 'W-statistic Shapiro-Wilk test': shapiro_wilk_w,\n",
    "                                                                                                    'p-value Shapiro-Wilk test': shapiro_wilk_p}, index=[0])], axis=0)\n",
    "                    df_z_wilcoxon_results = pd.concat([df_z_wilcoxon_results, pd.DataFrame({'Recon 1': recon, 'Recon 2': recon2, 'Test statistic Wilcoxon Signed Rank Test': wilcoxon_results[0],\n",
    "                                                                                            'p-value Wilcoxon Signed Rank Test': wilcoxon_results[1], 'Effect Size r': wilcoxon_results[2],\n",
    "                                                                                            'Median 1': wilcoxon_results[3], 'Median 2': wilcoxon_results[4]}, index=[0])], axis=0)\n",
    "        # print('T-test rel z-score results')\n",
    "        # print(df_z_ttest_rel_results)\n",
    "        # print('Shapiro-Wilk z-score results')\n",
    "        # print(df_z_shapiro_wilk_results)\n",
    "        # print('Wilcoxon z-score results')\n",
    "        # print(df_z_wilcoxon_results)\n",
    "\n",
    "        # combine df_z_ttest_rel_results, df_z_shapiro_wilk_results, df_z_wilcoxon_results into one dataframe \"stats_all\"\n",
    "        df_z_ttest_rel_results = df_z_ttest_rel_results.reset_index(\n",
    "            drop=True)\n",
    "        df_z_shapiro_wilk_results = df_z_shapiro_wilk_results.reset_index(\n",
    "            drop=True)\n",
    "        df_z_wilcoxon_results = df_z_wilcoxon_results.reset_index(drop=True)\n",
    "        stats_all = pd.concat(\n",
    "            [df_z_ttest_rel_results, df_z_shapiro_wilk_results, df_z_wilcoxon_results], axis=1)\n",
    "        # check if the p-value for the Shapiro-Wilk test is less than 0.05,\n",
    "        # if so, check if the p-value for the Wilcoxon test is less than 0.005,\n",
    "        # if so, then add a tally to the \"significant_result\" column. \n",
    "        # If the p-value for the Shapiro-Wilk test is greater than 0.05 and \n",
    "        # the p-value of the paired ttest is less than 0.005, then add a tally to the \"significant_result\" column.\n",
    "        stats_all['significant_result'] = 0\n",
    "        alpha_shapiro_wilk = 0.05\n",
    "        alpha_wilcoxon = 0.05 / 9\n",
    "        alpha_paired_ttest = 0.05 / 9\n",
    "        for index, row in stats_all.iterrows():\n",
    "            if float(row['p-value Shapiro-Wilk test']) < 0.05:\n",
    "                if float(row['p-value Wilcoxon Signed Rank Test']) < alpha_wilcoxon:\n",
    "                    stats_all.at[index, 'significant_result'] = 1\n",
    "            elif float(row['p-value Shapiro-Wilk test']) > 0.05:\n",
    "                if float(row['p-value Paired T-test']) < alpha_paired_ttest:\n",
    "                    stats_all.at[index, 'significant_result'] = 1\n",
    "        # print(stats_all)\n",
    "        stats_all.to_csv(main_path+'/stats/'+file.stem + '_walk_length_' +\n",
    "                         str(i)+'_stats_all.csv', index=False)\n",
    "        print(str(stats_all['significant_result'].sum())+' significant results out of '+str(len(stats_all['significant_result'])/2)+' total results')\n",
    "\n",
    "\n",
    "\n",
    "        # save dataframe to csv\n",
    "        df_z_ttest_rel_results.to_csv(\n",
    "            main_path+'/stats/'+file.stem+'_walk_length_'+str(i)+'_z_score_ttest_rel_results.csv', index=False)\n",
    "        df_z_shapiro_wilk_results.to_csv(\n",
    "            main_path+'/stats/'+file.stem+'_walk_length_'+str(i)+'_shapiro_wilk_results.csv', index=False)\n",
    "        df_z_wilcoxon_results.to_csv(\n",
    "            main_path+'/stats/'+file.stem+'_walk_length_'+str(i)+'_wilcoxon_results.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch16.csv\n",
      "volume_weighted_all_percent_batch16\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch11.csv\n",
      "count_all_percent_batch11\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch12.csv\n",
      "count_all_percent_batch12\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch13.csv\n",
      "volume_weighted_all_percent_batch13\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch17.csv\n",
      "volume_weighted_all_percent_batch17\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch15.csv\n",
      "count_all_percent_batch15\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch18.csv\n",
      "count_all_percent_batch18\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch15.csv\n",
      "volume_weighted_all_percent_batch15\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch19.csv\n",
      "count_all_percent_batch19\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch14.csv\n",
      "count_all_percent_batch14\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch17.csv\n",
      "count_all_percent_batch17\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch18.csv\n",
      "volume_weighted_all_percent_batch18\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch11.csv\n",
      "volume_weighted_all_percent_batch11\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch13.csv\n",
      "count_all_percent_batch13\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch10.csv\n",
      "volume_weighted_all_percent_batch10\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch14.csv\n",
      "volume_weighted_all_percent_batch14\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch19.csv\n",
      "volume_weighted_all_percent_batch19\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch10.csv\n",
      "count_all_percent_batch10\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch16.csv\n",
      "count_all_percent_batch16\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch12.csv\n",
      "volume_weighted_all_percent_batch12\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "\n",
    "file_list = Path(main_path).glob('*batch??.csv')\n",
    "for file in file_list:\n",
    "    print(file)\n",
    "    df = pd.read_csv(file)\n",
    "    print(file.stem)\n",
    "    # melt data for boxplot\n",
    "    dd=pd.melt(df,id_vars=['Recon'],value_vars=['1','2','3','4','5'],var_name='Walk Length')\n",
    "    # seaborn boxplot with hue based on recon method\n",
    "    sns.boxplot(x='Walk Length',y='value',data=dd,hue='Recon')\n",
    "    # plt.show()\n",
    "    plt.ylabel('Pearson Score')\n",
    "    # save figure\n",
    "    plt.savefig(main_path+'/plots/'+file.stem+'_box_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing Number of Significant Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of significant paired differences Streamline Count: DTI vs MSMT\n",
      "0.35\n",
      "Fraction of significant paired differences Volume Weighted Streamline Count: DTI vs MSMT\n",
      "0.9\n",
      "Fraction of significant paired differences Streamline Count: GQI vs MSMT\n",
      "0.35\n",
      "Fraction of significant paired differences Volume Weighted Streamline Count: GQI vs MSMT\n",
      "0.95\n",
      "Fraction of significant paired differences Streamline Count: DTI vs GQI\n",
      "0.15\n",
      "Fraction of significant paired differences Volume Weighted Streamline Count: DTI vs GQI\n",
      "0.35\n"
     ]
    }
   ],
   "source": [
    "# read in all stats_all files from walk length 4\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats'\n",
    "file_list = Path(main_path).glob('*walk_length_4_stats_all.csv')\n",
    "df_list_count = []\n",
    "df_list_vol = []\n",
    "# df_list_mean_length = []\n",
    "for file in file_list:\n",
    "    if 'count' in file.stem:\n",
    "        df_list_count.append(pd.read_csv(file))\n",
    "    elif 'vol' in file.stem:\n",
    "        df_list_vol.append(pd.read_csv(file))\n",
    "    # elif 'mean_length' in file.stem:\n",
    "    #     df_list_mean_length.append(pd.read_csv(file))\n",
    "df_count = pd.concat(df_list_count)\n",
    "df_vol = pd.concat(df_list_vol)\n",
    "# df_mean_length = pd.concat(df_list_mean_length)\n",
    "\n",
    "# define a function to calculate the Fraction of significant results for each Recon 1 vs Recon 2 method comparison\n",
    "# where the value of 'Recon 1' contains recon1 and of 'Recon 2' contains recon2\n",
    "def sig_frac(df, recon1, recon2):\n",
    "    df = df[(df['Recon 1'].str.contains(recon1)) & (df['Recon 2'].str.contains(recon2))]\n",
    "    sig_frac = df['significant_result'].sum() / (len(df['significant_result']))\n",
    "    return sig_frac\n",
    "\n",
    "# Calculate the Fraction of significant results for each Recon 1 vs Recon 2 method comparison\n",
    "# for count where the value of 'Recon 1' contains DTI and of 'Recon 2' contains MSMT\n",
    "sig_frac_count_dm = sig_frac(df_count, 'DTI', 'MSMT')\n",
    "print(\"Fraction of significant paired differences Streamline Count: DTI vs MSMT\")\n",
    "print(sig_frac_count_dm)\n",
    "# for vol where the value of 'Recon 1' contains DTI and of 'Recon 2' contains MSMT\n",
    "sig_frac_vol_dm = sig_frac(df_vol, 'DTI', 'MSMT')\n",
    "print(\"Fraction of significant paired differences Volume Weighted Streamline Count: DTI vs MSMT\")\n",
    "print(sig_frac_vol_dm)\n",
    "# for mean length where the value of 'Recon 1' contains DTI and of 'Recon 2' contains MSMT\n",
    "# sig_frac_mean_length_dm = sig_frac(df_mean_length, 'DTI', 'MSMT')\n",
    "# print(\"Fraction of paired significant differences Mean Length: DTI vs MSMT\")\n",
    "# print(sig_frac_mean_length_dm)\n",
    "sig_frac_count_gm = sig_frac(df_count, 'GQI', 'MSMT')\n",
    "print(\"Fraction of significant paired differences Streamline Count: GQI vs MSMT\")\n",
    "print(sig_frac_count_gm)\n",
    "sig_frac_vol_gm = sig_frac(df_vol, 'GQI', 'MSMT')\n",
    "print(\"Fraction of significant paired differences Volume Weighted Streamline Count: GQI vs MSMT\")\n",
    "print(sig_frac_vol_gm)\n",
    "# sig_frac_mean_length_gm = sig_frac(df_mean_length, 'GQI', 'MSMT')\n",
    "# print(\"Fraction of paired significant differences Mean Length: GQI vs MSMT\")\n",
    "# print(sig_frac_mean_length_gm)\n",
    "sig_frac_count_dg = sig_frac(df_count, 'DTI', 'GQI')\n",
    "print(\"Fraction of significant paired differences Streamline Count: DTI vs GQI\")\n",
    "print(sig_frac_count_dg)\n",
    "sig_frac_vol_dg = sig_frac(df_vol, 'DTI', 'GQI')\n",
    "print(\"Fraction of significant paired differences Volume Weighted Streamline Count: DTI vs GQI\")\n",
    "print(sig_frac_vol_dg)\n",
    "# sig_frac_mean_length_dg = sig_frac(df_mean_length, 'DTI', 'GQI')\n",
    "# print(\"Fraction of paired significant differences Mean Length: DTI vs GQI\")\n",
    "# print(sig_frac_mean_length_dg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619693/2977612436.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_619693/2977612436.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_619693/2977612436.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_619693/2977612436.py:28: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_619693/2977612436.py:37: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_619693/2977612436.py:38: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_619693/2977612436.py:45: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_vol_ttest = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n"
     ]
    }
   ],
   "source": [
    "# read in z-score paired t-test results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('volume_*_walk_length_4_z_score_ttest_rel_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Node Volume Weighted Streamline Count', recon2='GQI Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_min.csv')\n",
    "\n",
    "# read in z-score paired t-test results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "df_vol_ttest = pd.DataFrame()\n",
    "df_vol_ttest = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_vol_ttest.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_vol_ttest.to_csv(main_path+'/stats/walk_4_volume_weighted_paired_ttest_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619693/127232398.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_619693/127232398.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_619693/127232398.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_619693/127232398.py:28: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_619693/127232398.py:37: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_619693/127232398.py:38: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_619693/127232398.py:45: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_vol_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n"
     ]
    }
   ],
   "source": [
    "# read in z-score wilcoxon signed rank test results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('volume_*_walk_length_4_wilcoxon_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Node Volume Weighted Streamline Count', recon2='GQI Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_min.csv')\n",
    "\n",
    "# combine all volume weighted wilcoxon signed rank test results at walk length 4\n",
    "verbose = False\n",
    "df_vol_wil = pd.DataFrame()\n",
    "df_vol_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_vol_wil.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_vol_wil.to_csv(main_path+'/stats/walk_4_volume_weighted_wilcoxon_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619693/4074295172.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_619693/4074295172.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_619693/4074295172.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_619693/4074295172.py:28: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_619693/4074295172.py:37: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_619693/4074295172.py:38: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_619693/4074295172.py:46: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_count_ttest = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n"
     ]
    }
   ],
   "source": [
    "# read in z-score paired t-test results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('count_*_walk_length_4_z_score_ttest_rel_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Streamline Count', recon2='GQI Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_min.csv')\n",
    "\n",
    "\n",
    "# combine all count t-test results at walk length 4\n",
    "verbose = False\n",
    "df_count_ttest = pd.DataFrame()\n",
    "df_count_ttest = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_count_ttest.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_count_ttest.to_csv(main_path+'/stats/walk_4_count_paired_ttest_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619693/1331690065.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_619693/1331690065.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_619693/1331690065.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_619693/1331690065.py:28: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_619693/1331690065.py:37: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_619693/1331690065.py:38: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_619693/1331690065.py:45: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_count_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n"
     ]
    }
   ],
   "source": [
    "# read in z-score wilcoxon signed rank test results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('count_*_walk_length_4_wilcoxon_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Streamline Count', recon2='GQI Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_min.csv')\n",
    "\n",
    "# combine all volume weighted wilcoxon signed rank test results at walk length 4\n",
    "verbose = False\n",
    "df_count_wil = pd.DataFrame()\n",
    "df_count_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_count_wil.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_count_wil.to_csv(main_path+'/stats/walk_4_count_wilcoxon_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619693/248381940.py:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:24: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:25: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:32: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:33: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:65: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_count_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:66: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_count_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:73: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_count_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:74: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_count_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:81: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_count_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_619693/248381940.py:82: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_count_shapiro_wilk_results_std.csv')\n"
     ]
    }
   ],
   "source": [
    "# read in shapiro-wilk results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/'\n",
    "\n",
    "shapiro_file_list = Path(main_path).glob('volume_*_walk_length_4_shapiro_wilk_results.csv')\n",
    "df_sh = pd.DataFrame()\n",
    "for sh_file in shapiro_file_list:\n",
    "    if verbose:\n",
    "        print(sh_file)\n",
    "    df_sh = pd.concat([df_sh,pd.read_csv(sh_file)],axis=0)\n",
    "df_sh_dm = get_df_int(df=df_sh, recon1='DTI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_gm = get_df_int(df=df_sh, recon1='GQI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_gm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_mean.csv')\n",
    "df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_dq = get_df_int(df=df_sh, recon1='DTI Node Volume Weighted Streamline Count', recon2='GQI Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dq)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_std.csv')\n",
    "\n",
    "# read in mean and std files for shapiro-wilk at walk 4, summarize results for different recon1 reecon2 combinations\n",
    "verbose = False\n",
    "shapiro_list = ['walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_std.csv']\n",
    "df_summary = pd.DataFrame()\n",
    "for s_file in shapiro_list:\n",
    "    if verbose:\n",
    "        print(s_file)\n",
    "    df = pd.read_csv(main_path+'/'+s_file, index_col=0)\n",
    "    df_summary = pd.concat([df_summary,df],axis=1)\n",
    "\n",
    "df_summary.columns = ['DTI MSMT mean', 'DTI MSMT std', 'DTI GQI mean', 'DTI GQI std', 'GQI MSMT mean', 'GQI MSMT std']\n",
    "df_summary.to_csv(main_path+'/walk_4_volume_weighted_shapiro_wilk_summary.csv')\n",
    "\n",
    "\n",
    "shapiro_file_list = Path(main_path).glob('count_*_walk_length_4_shapiro_wilk_results.csv')\n",
    "df_sh = pd.DataFrame()\n",
    "for sh_file in shapiro_file_list:\n",
    "    if verbose:\n",
    "        print(sh_file)\n",
    "    df_sh = pd.concat([df_sh,pd.read_csv(sh_file)],axis=0)\n",
    "df_sh_dm = get_df_int(df=df_sh, recon1='DTI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_count_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_count_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_gm = get_df_int(df=df_sh, recon1='GQI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_gm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_count_shapiro_wilk_results_mean.csv')\n",
    "df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_count_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_dq = get_df_int(df=df_sh, recon1='DTI Streamline Count', recon2='GQI Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dq)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_count_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_count_shapiro_wilk_results_std.csv')\n",
    "\n",
    "# read in mean and std files for shapiro-wilk at walk 4, summarize results for different recon1 reecon2 combinations\n",
    "verbose = False\n",
    "shapiro_list = ['walk_4_DTI_msmt_count_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_msmt_count_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_DTI_GQI_count_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_GQI_count_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_GQI_msmt_count_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_GQI_msmt_count_shapiro_wilk_results_std.csv']\n",
    "df_summary = pd.DataFrame()\n",
    "for s_file in shapiro_list:\n",
    "    if verbose:\n",
    "        print(s_file)\n",
    "    df = pd.read_csv(main_path+'/'+s_file, index_col=0)\n",
    "    df_summary = pd.concat([df_summary,df],axis=1)\n",
    "\n",
    "df_summary.columns = ['DTI MSMT mean', 'DTI MSMT std', 'DTI GQI mean', 'DTI GQI std', 'GQI MSMT mean', 'GQI MSMT std']\n",
    "df_summary.to_csv(main_path+'/walk_4_count_shapiro_wilk_summary.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in z-score paired t-test results from all batches in main_path/stats with walk length 4\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('count_*_walk_length_4_z_score_ttest_rel_results.csv')\n",
    "df = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    print(file)\n",
    "    df = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is GQI Streamline Count and Recon 2 is MSMT CSD SIFT2 Streamline Count\n",
    "print('mean')\n",
    "print(df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].mean())\n",
    "print('std')\n",
    "print(df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].std())\n",
    "print('max')\n",
    "print(df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].max())\n",
    "print('min')\n",
    "print(df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].min())\n",
    "\n",
    "# save out mean, std, max, min to csv\n",
    "df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].mean().to_csv(main_path+'/stats/walk_4_gqi_msmt_count_paired_ttest_results_mean.csv')\n",
    "df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].std().to_csv(main_path+'/stats/walk_4_gqi_msmt_count_paired_ttest_results_std.csv')\n",
    "df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].max().to_csv(main_path+'/stats/walk_4_gqi_msmt_count_paired_ttest_results_max.csv')\n",
    "df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].min().to_csv(main_path+'/stats/walk_4_gqi_msmt_count_paired_ttest_results_min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in paired t-test results from all batches in main_path/stats with walk length 4\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('mean_length_*_walk_length_4_ttest_rel_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Mean Length', recon2='MSMT CSD Mean Length', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_wilcoxon_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_wilcoxon_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_wilcoxon_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_wilcoxon_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Mean Length', recon2='MSMT CSD Mean Length', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_wilcoxon_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_wilcoxon_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_wilcoxon_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_wilcoxon_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Mean Length', recon2='GQI Mean Length', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_wilcoxon_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_wilcoxon_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_wilcoxon_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_wilcoxon_results_min.csv')\n",
    "\n",
    "# combine all volume weighted wilcoxon signed rank test results at walk length 4\n",
    "verbose = False\n",
    "df_ml_wil = pd.DataFrame()\n",
    "df_ml_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_ml_wil.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_ml_wil.to_csv(main_path+'/stats/walk_4_mean_length_wilcoxon_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_file_list = Path(main_path).glob('mean_path_length_*_walk_length_4_shapiro_wilk_results.csv')\n",
    "df_sh = pd.DataFrame()\n",
    "for sh_file in shapiro_file_list:\n",
    "    if verbose:\n",
    "        print(sh_file)\n",
    "    df_sh = pd.concat([df_sh,pd.read_csv(sh_file)],axis=0)\n",
    "df_sh_dm = get_df_int(df=df_sh, recon1='DTI Mean Length', recon2='MSMT CSD Mean Length', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_mean_length_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_mean_length_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_gm = get_df_int(df=df_sh, recon1='GQI Mean Length', recon2='MSMT CSD Mean Length', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_gm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_mean_length_shapiro_wilk_results_mean.csv')\n",
    "df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_mean_length_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_dq = get_df_int(df=df_sh, recon1='DTI Mean Length', recon2='GQI Mean Length', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dq)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_mean_length_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_mean_length_shapiro_wilk_results_std.csv')\n",
    "\n",
    "# read in mean and std files for shapiro-wilk at walk 4, summarize results for different recon1 reecon2 combinations\n",
    "verbose = False\n",
    "shapiro_list = ['walk_4_DTI_msmt_mean_length_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_msmt_mean_length_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_DTI_GQI_mean_length_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_GQI_mean_length_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_GQI_msmt_mean_length_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_GQI_msmt_mean_length_shapiro_wilk_results_std.csv']\n",
    "df_summary = pd.DataFrame()\n",
    "for s_file in shapiro_list:\n",
    "    if verbose:\n",
    "        print(s_file)\n",
    "    df = pd.read_csv(main_path+'/'+s_file, index_col=0)\n",
    "    df_summary = pd.concat([df_summary,df],axis=1)\n",
    "\n",
    "df_summary.columns = ['DTI MSMT mean', 'DTI MSMT std', 'DTI GQI mean', 'DTI GQI std', 'GQI MSMT mean', 'GQI MSMT std']\n",
    "df_summary.to_csv(main_path+'/walk_4_mean_length_shapiro_wilk_summary.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
