{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# create a function for performing the paired t-test \n",
    "#     and for calculating the effect size of the paired t-test\n",
    "#     and also returning the means of the two samples \n",
    "def paired_ttest(x,y,alternative='two-sided'):\n",
    "    x_arr = np.array(x)\n",
    "    y_arr = np.array(y)\n",
    "    # check for number of outliers in difference between two arrays\n",
    "    #    based on a threshold of differences IQD * 3 and remove outliers \n",
    "    #    from both arrays and add number of outliers to output\n",
    "    # differences = x_arr - y_arr\n",
    "    # outliers = []\n",
    "    # for i in range(len(x_arr)):\n",
    "    #    if abs(x_arr[i] - y_arr[i]) > (3 * (np.percentile(differences,75) - np.percentile(differences,25))):\n",
    "    #       outliers.append(i)\n",
    "    # number_of_outliers = len(outliers)\n",
    "    # print(\"Number of outliers removed: \" + str(number_of_outliers))\n",
    "    # x_arr = np.delete(x_arr,outliers)\n",
    "    # y_arr = np.delete(y_arr,outliers)    \n",
    "    # paired t-test from scipy.stats\n",
    "    t_statistic, p_value = ttest_rel(x_arr,y_arr,alternative=alternative)\n",
    "    # Calculate effect size (Cohen's d)\n",
    "    # calculate mean of differences between two arrays\n",
    "    mean_diff = np.mean(x_arr - y_arr)\n",
    "    mean_x = np.mean(x_arr)\n",
    "    mean_y = np.mean(y_arr)\n",
    "    # calculate cohen's d' corrected for related samples\n",
    "    # calculate standard deviation of differences between two arrays\n",
    "    differences = x_arr - y_arr\n",
    "    std_diff = np.std(differences)\n",
    "    # calculate cohen's d\n",
    "    d = mean_diff / std_diff\n",
    "    # calculate hedge's g\n",
    "    g = d * (1 - (3 / (4 * (len(x_arr) - 2) - 1)))\n",
    "    # pooled_std = np.sqrt((np.std(x_arr) ** 2 + np.std(y_arr) ** 2) / 2)\n",
    "    # d = mean_diff / pooled_std\n",
    "    # if sample size is < 20, calculate hedge's g instead of cohen's d\n",
    "    #    hedge's g is a correction for small sample sizes\n",
    "    # if len(x_arr) < 20:\n",
    "    #     g = d * (1 - (3 / (4 * (len(x_arr) - 2) - 1)))\n",
    "    #     return t_statistic, p_value, g, mean_x, mean_y, number_of_outliers\n",
    "    return t_statistic, p_value, g, mean_x, mean_y\n",
    "\n",
    "# create a function for performing the shapiro-wilk test for normality\n",
    "#     of differences between two paired samples and for creating a qq plot\n",
    "#     of the differences between the two paired samples\n",
    "def shapiro_wilk_qq(x,y,recon1,recon2,walk_length):\n",
    "    # shapiro-wilk test for normality\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(x-y)\n",
    "    # # make figure\n",
    "    # plt.figure(figsize=(8,6))\n",
    "    # # qq plot\n",
    "    # qq = stats.probplot(x-y, dist=\"norm\", plot=plt)\n",
    "    # # plot name\n",
    "    # # qq_plot_name = '/datain/dataset/plots/normality/qq_plot_differences'+str(x)+'_'+str(y)+'.png' # Uncomment for use in singularity\n",
    "    # qq_plot_name = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/plots/normality/qq_plot_differences'+str(recon1)+'_'+str(recon2)+'_walklength_'+str(walk_length)+'.png'\n",
    "    # # make matplotlib plot\n",
    "    # plt.title('QQ Plot for Differences between '+str(recon1)+' and '+str(recon2))\n",
    "    # plt.xlabel('Theoretical Quantiles')\n",
    "    # plt.ylabel('Ordered Values')\n",
    "    # plt.tight_layout()\n",
    "    # # save qq plot\n",
    "    # plt.savefig(qq_plot_name)\n",
    "    # plt.close()\n",
    "    return shapiro_stat, shapiro_p\n",
    "\n",
    "# create a function for performing the wilcoxon signed rank test with paired samples \n",
    "#     and calculating the effect size of the paired samples wilcoxon signed rank test\n",
    "#     and also returning the medians of the two samples\n",
    "def wilcoxon_test(x,y,alternative='two-sided'):\n",
    "    x_arr = np.array(x)\n",
    "    y_arr = np.array(y)\n",
    "    w_statistic, p_value = stats.wilcoxon(x_arr,y_arr,alternative=alternative,zero_method='pratt',mode='exact')\n",
    "    median_x = np.median(x_arr)\n",
    "    median_y = np.median(y_arr)\n",
    "    # Calculate effect size (r)\n",
    "    r = w_statistic / (len(x_arr) * (len(y_arr) + 1) / 2)\n",
    "    return w_statistic, p_value, r, median_x, median_y\n",
    "\n",
    "\n",
    "# create a function that reads a dataframe and returns a dataframe with the\n",
    "#     values of column i for which the row values in the 'Recon 1' column equal to recon1 \n",
    "#     and the values in the 'Recon 2' column equal to recon2\n",
    "def get_df_int(df,recon1,recon2,i):\n",
    "    if i is not None:\n",
    "        df_i = df[(df['Recon 1'][i] == recon1) & (df['Recon 2'][i] == recon2)]\n",
    "    else:\n",
    "        df_i = df[(df['Recon 1'] == recon1) & (df['Recon 2'] == recon2)]\n",
    "    return df_i\n",
    "\n",
    "# # create a function for calculating the mean, standard deviation, and median of the\n",
    "# #     values in each column of a dataframe - where the 'Recon 1' is equal to recon1\n",
    "# #     and 'Recon 2' is equal to recon2\n",
    "# def get_stats(df,recon1,recon2):\n",
    "#     df_i = df.loc[(df['Recon 1'] == recon1) & (df['Recon 2'] == recon2)]\n",
    "#     mean = df_i.mean(axis=0)\n",
    "#     std = df_i.std(axis=0)\n",
    "#     median = df_i.median(axis=0)\n",
    "#     return mean, std, median\n",
    "\n",
    "def print_stats(df):\n",
    "    print('Mean:')\n",
    "    print(df.mean(axis=0))\n",
    "    print('Standard Deviation:')\n",
    "    print(df.std(axis=0))\n",
    "    print('Median:')\n",
    "    print(df.median(axis=0))\n",
    "    print('Min:')\n",
    "    print(df.min(axis=0))\n",
    "    print('Max:')\n",
    "    print(df.max(axis=0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in files and perform statistical tests of independence for Pearson scores at each walk length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/numpy/lib/nanfunctions.py:1112: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/paul/.local/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:1956: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  cond2 = (x >= np.asarray(_b)) & cond0\n",
      "/home/paul/.local/lib/python3.8/site-packages/scipy/stats/morestats.py:3158: RuntimeWarning: invalid value encountered in greater\n",
      "  r_plus = np.sum((d > 0) * r)\n",
      "/home/paul/.local/lib/python3.8/site-packages/scipy/stats/morestats.py:3159: RuntimeWarning: invalid value encountered in less\n",
      "  r_minus = np.sum((d < 0) * r)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/numpy/lib/nanfunctions.py:1112: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/paul/.local/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:1956: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  cond2 = (x >= np.asarray(_b)) & cond0\n",
      "/home/paul/.local/lib/python3.8/site-packages/scipy/stats/morestats.py:3158: RuntimeWarning: invalid value encountered in greater\n",
      "  r_plus = np.sum((d > 0) * r)\n",
      "/home/paul/.local/lib/python3.8/site-packages/scipy/stats/morestats.py:3159: RuntimeWarning: invalid value encountered in less\n",
      "  r_minus = np.sum((d < 0) * r)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "0 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "1 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "3 significant results out of 3.0 total results\n",
      "2 significant results out of 3.0 total results\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "verbose = False\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path).glob('*batch??.csv')\n",
    "for file in file_list:\n",
    "    if verbose is True:\n",
    "        print(file)\n",
    "    df = pd.read_csv(file)\n",
    "    # ,ave to a copy of df, then perform Fisher's r to z transformation on Pearson scores\n",
    "    df_z = df.copy()\n",
    "    for i in range(1, 6):\n",
    "        df_z[str(i)] = np.arctanh(df_z[str(i)])\n",
    "    # print(df_z)\n",
    "    if verbose is True:\n",
    "        print(file.stem)\n",
    "    # compare difference in score between Recon methods for each walk length column\n",
    "    for i in range(1, 6):\n",
    "        if verbose is True:\n",
    "            print('Walk Length: '+str(i))\n",
    "        # get unique values of Recon column\n",
    "        recon_list = df['Recon'].unique()\n",
    "        df_stats_all = pd.DataFrame()\n",
    "        df_stats = pd.DataFrame()\n",
    "        # loop through each unique value of Recon column\n",
    "        for recon in recon_list:\n",
    "            if verbose is True:\n",
    "                print(recon)\n",
    "            # combine statistics from each Recon method into dataframe\n",
    "            df_stats = pd.DataFrame({'Recon': recon_list, 'Mean': df[df['Recon'] == str(recon)][str(i)].mean(), 'Stdev': df[df['Recon'] == str(recon)][str(i)].std(\n",
    "            ), 'Median': df[df['Recon'] == str(recon)][str(i)].median(), 'IQR': df[df['Recon'] == str(recon)][str(i)].quantile(q=0.75)-df[df['Recon'] == str(recon)][str(i)].quantile(q=0.25)})\n",
    "            # combine df_stats from each Recon method into dataframe\n",
    "            df_stats_all = pd.concat([df_stats, df_stats_all], axis=0)\n",
    "        # print(df_stats_all)\n",
    "        # save dataframe to csv\n",
    "        df_stats_all.to_csv(main_path+'/stats/'+file.stem +\n",
    "                            '_walk_length_'+str(i)+'_stats.csv', index=False)\n",
    "        # calculate p-value for difference in score between Recon methods in recon_list, with comparisons between each pair of Recon methods, save all statistics and p-values to csv\n",
    "        df_z_ttest_rel_results = pd.DataFrame()\n",
    "        df_z_shapiro_wilk_results = pd.DataFrame()\n",
    "        df_z_wilcoxon_results = pd.DataFrame()\n",
    "        for recon in recon_list:\n",
    "            for recon2 in recon_list:\n",
    "                if recon != recon2:\n",
    "                    if verbose is True:\n",
    "                        print(recon)\n",
    "                        print(recon2)\n",
    "                    # calculate t-test on z-scores, get t-statistic and p-value and effect size, means\n",
    "                    ttest_rel_z_score = paired_ttest(df_z[df_z['Recon'] == str(\n",
    "                        recon)][str(i)], df_z[df_z['Recon'] == str(recon2)][str(i)], alternative='less')\n",
    "                    if verbose:\n",
    "                        print(ttest_rel_z_score[2])\n",
    "                    # calculate shapiro-wilk test on z-scores, get W-statistic and p-value\n",
    "                    # get array of df_z values for recon and recon2\n",
    "                    recon_1_z_scores = np.array(df_z[df_z['Recon'] == str(recon)][str(i)])\n",
    "                    recon_2_z_scores = np.array(df_z[df_z['Recon'] == str(recon2)][str(i)])\n",
    "                    # calculate differences between arrays of recon and recon2 z-scores\n",
    "                    differences = recon_1_z_scores - recon_2_z_scores\n",
    "                    shapiro_wilk_w, shapiro_wilk_p = stats.shapiro(differences)\n",
    "                    if verbose:\n",
    "                        print(shapiro_wilk_p)\n",
    "                    # calculate wilcoxon test on z-scores, get test statistic and p-value and effect size, medians\n",
    "                    wilcoxon_results = wilcoxon_test(df_z[df_z['Recon'] == str(\n",
    "                        recon)][str(i)], df_z[df_z['Recon'] == str(recon2)][str(i)], alternative='less')\n",
    "                    # combine stats into dataframe\n",
    "                    df_z_ttest_rel_results = pd.concat([df_z_ttest_rel_results, pd.DataFrame({'Recon 1': recon, 'Recon 2': recon2, 'T-statistic Paired T-test': ttest_rel_z_score[0],\n",
    "                                                                                                'p-value Paired T-test': ttest_rel_z_score[1], 'Effect Size d': ttest_rel_z_score[2],\n",
    "                                                                                                'Mean 1': ttest_rel_z_score[3], 'Mean 2': ttest_rel_z_score[4]}, index=[0])], axis=0)\n",
    "                    df_z_shapiro_wilk_results = pd.concat([df_z_shapiro_wilk_results, pd.DataFrame({'Recon 1': recon, 'Recon 2': recon2, 'W-statistic Shapiro-Wilk test': shapiro_wilk_w,\n",
    "                                                                                                    'p-value Shapiro-Wilk test': shapiro_wilk_p}, index=[0])], axis=0)\n",
    "                    df_z_wilcoxon_results = pd.concat([df_z_wilcoxon_results, pd.DataFrame({'Recon 1': recon, 'Recon 2': recon2, 'Test statistic Wilcoxon Signed Rank Test': wilcoxon_results[0],\n",
    "                                                                                            'p-value Wilcoxon Signed Rank Test': wilcoxon_results[1], 'Effect Size r': wilcoxon_results[2],\n",
    "                                                                                            'Median 1': wilcoxon_results[3], 'Median 2': wilcoxon_results[4]}, index=[0])], axis=0)\n",
    "        # print('T-test rel z-score results')\n",
    "        # print(df_z_ttest_rel_results)\n",
    "        # print('Shapiro-Wilk z-score results')\n",
    "        # print(df_z_shapiro_wilk_results)\n",
    "        # print('Wilcoxon z-score results')\n",
    "        # print(df_z_wilcoxon_results)\n",
    "\n",
    "        # combine df_z_ttest_rel_results, df_z_shapiro_wilk_results, df_z_wilcoxon_results into one dataframe \"stats_all\"\n",
    "        df_z_ttest_rel_results = df_z_ttest_rel_results.reset_index(\n",
    "            drop=True)\n",
    "        df_z_shapiro_wilk_results = df_z_shapiro_wilk_results.reset_index(\n",
    "            drop=True)\n",
    "        df_z_wilcoxon_results = df_z_wilcoxon_results.reset_index(drop=True)\n",
    "        stats_all = pd.concat(\n",
    "            [df_z_ttest_rel_results, df_z_shapiro_wilk_results, df_z_wilcoxon_results], axis=1)\n",
    "        # check if the p-value for the Shapiro-Wilk test is less than 0.05,\n",
    "        # if so, check if the p-value for the Wilcoxon test is less than 0.005,\n",
    "        # if so, then add a tally to the \"significant_result\" column. \n",
    "        # If the p-value for the Shapiro-Wilk test is greater than 0.05 and \n",
    "        # the p-value of the paired ttest is less than 0.005, then add a tally to the \"significant_result\" column.\n",
    "        stats_all['significant_result'] = 0\n",
    "        # make lists for getting the average of test statistic and p-value for paired ttest and wilcoxon\n",
    "        # 'Average T-Statistic (Paired T-test)'\n",
    "        # 'Average p-value (Paired T-test)'\n",
    "        # 'Average Effect Size (Cohens d)'\n",
    "        # 'Average W-Statistic (Wilcoxon SR Test)'\n",
    "        # 'Average p-value (Wilcoxon SR Test)'\n",
    "        # 'Average Effect Size (Wilcoxon r)'\n",
    "        alpha_shapiro_wilk = 0.05\n",
    "        alpha_wilcoxon = 0.05 / 9\n",
    "        alpha_paired_ttest = 0.05 / 9\n",
    "        for index, row in stats_all.iterrows():\n",
    "            if float(row['p-value Shapiro-Wilk test']) < 0.05:\n",
    "                if float(row['p-value Wilcoxon Signed Rank Test']) < alpha_wilcoxon:\n",
    "                    stats_all.at[index, 'significant_result'] = 1\n",
    "            elif float(row['p-value Shapiro-Wilk test']) > 0.05:\n",
    "                if float(row['p-value Paired T-test']) < alpha_paired_ttest:\n",
    "                    stats_all.at[index, 'significant_result'] = 1\n",
    "        # print(stats_all)\n",
    "        stats_all.to_csv(main_path+'/stats/'+file.stem + '_walk_length_' +\n",
    "                         str(i)+'_stats_all.csv', index=False)\n",
    "        print(str(stats_all['significant_result'].sum())+' significant results out of '+str(len(stats_all['significant_result'])/2)+' total results')\n",
    "\n",
    "\n",
    "\n",
    "        # save dataframe to csv\n",
    "        df_z_ttest_rel_results.to_csv(\n",
    "            main_path+'/stats/'+file.stem+'_walk_length_'+str(i)+'_z_score_ttest_rel_results.csv', index=False)\n",
    "        df_z_shapiro_wilk_results.to_csv(\n",
    "            main_path+'/stats/'+file.stem+'_walk_length_'+str(i)+'_shapiro_wilk_results.csv', index=False)\n",
    "        df_z_wilcoxon_results.to_csv(\n",
    "            main_path+'/stats/'+file.stem+'_walk_length_'+str(i)+'_wilcoxon_results.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch24.csv\n",
      "count_all_percent_batch24\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch33.csv\n",
      "count_all_percent_batch33\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch39.csv\n",
      "count_all_percent_batch39\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch47.csv\n",
      "volume_weighted_all_percent_batch47\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch16.csv\n",
      "volume_weighted_all_percent_batch16\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch11.csv\n",
      "count_all_percent_batch11\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch42.csv\n",
      "count_all_percent_batch42\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch19.csv\n",
      "mean_path_length_all_percent_batch19\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch21.csv\n",
      "count_all_percent_batch21\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch24.csv\n",
      "mean_path_length_all_percent_batch24\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch28.csv\n",
      "count_all_percent_batch28\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch44.csv\n",
      "mean_path_length_all_percent_batch44\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch24.csv\n",
      "volume_weighted_all_percent_batch24\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch39.csv\n",
      "volume_weighted_all_percent_batch39\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch18.csv\n",
      "mean_path_length_all_percent_batch18\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch25.csv\n",
      "volume_weighted_all_percent_batch25\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch48.csv\n",
      "volume_weighted_all_percent_batch48\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch12.csv\n",
      "count_all_percent_batch12\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch13.csv\n",
      "volume_weighted_all_percent_batch13\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch33.csv\n",
      "volume_weighted_all_percent_batch33\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch44.csv\n",
      "volume_weighted_all_percent_batch44\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch45.csv\n",
      "mean_path_length_all_percent_batch45\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch13.csv\n",
      "mean_path_length_all_percent_batch13\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch34.csv\n",
      "volume_weighted_all_percent_batch34\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch17.csv\n",
      "volume_weighted_all_percent_batch17\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch36.csv\n",
      "volume_weighted_all_percent_batch36\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch22.csv\n",
      "volume_weighted_all_percent_batch22\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch41.csv\n",
      "mean_path_length_all_percent_batch41\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch21.csv\n",
      "volume_weighted_all_percent_batch21\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch31.csv\n",
      "mean_path_length_all_percent_batch31\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch15.csv\n",
      "mean_path_length_all_percent_batch15\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch12.csv\n",
      "mean_path_length_all_percent_batch12\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch22.csv\n",
      "mean_path_length_all_percent_batch22\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch27.csv\n",
      "count_all_percent_batch27\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch36.csv\n",
      "mean_path_length_all_percent_batch36\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch30.csv\n",
      "mean_path_length_all_percent_batch30\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch49.csv\n",
      "count_all_percent_batch49\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch26.csv\n",
      "count_all_percent_batch26\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch41.csv\n",
      "volume_weighted_all_percent_batch41\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch45.csv\n",
      "count_all_percent_batch45\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch41.csv\n",
      "count_all_percent_batch41\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch32.csv\n",
      "mean_path_length_all_percent_batch32\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch35.csv\n",
      "volume_weighted_all_percent_batch35\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch33.csv\n",
      "mean_path_length_all_percent_batch33\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch15.csv\n",
      "count_all_percent_batch15\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch30.csv\n",
      "volume_weighted_all_percent_batch30\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch26.csv\n",
      "volume_weighted_all_percent_batch26\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch34.csv\n",
      "count_all_percent_batch34\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch46.csv\n",
      "count_all_percent_batch46\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch43.csv\n",
      "mean_path_length_all_percent_batch43\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch18.csv\n",
      "count_all_percent_batch18\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch23.csv\n",
      "count_all_percent_batch23\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch29.csv\n",
      "mean_path_length_all_percent_batch29\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch49.csv\n",
      "mean_path_length_all_percent_batch49\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch32.csv\n",
      "volume_weighted_all_percent_batch32\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch44.csv\n",
      "count_all_percent_batch44\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch25.csv\n",
      "count_all_percent_batch25\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch46.csv\n",
      "volume_weighted_all_percent_batch46\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch49.csv\n",
      "volume_weighted_all_percent_batch49\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch20.csv\n",
      "count_all_percent_batch20\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch47.csv\n",
      "mean_path_length_all_percent_batch47\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch31.csv\n",
      "count_all_percent_batch31\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch15.csv\n",
      "volume_weighted_all_percent_batch15\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch37.csv\n",
      "count_all_percent_batch37\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch14.csv\n",
      "mean_path_length_all_percent_batch14\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch40.csv\n",
      "count_all_percent_batch40\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch35.csv\n",
      "count_all_percent_batch35\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch38.csv\n",
      "mean_path_length_all_percent_batch38\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch19.csv\n",
      "count_all_percent_batch19\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch14.csv\n",
      "count_all_percent_batch14\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch11.csv\n",
      "mean_path_length_all_percent_batch11\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch17.csv\n",
      "count_all_percent_batch17\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch35.csv\n",
      "mean_path_length_all_percent_batch35\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch29.csv\n",
      "volume_weighted_all_percent_batch29\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch20.csv\n",
      "volume_weighted_all_percent_batch20\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch43.csv\n",
      "count_all_percent_batch43\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch23.csv\n",
      "mean_path_length_all_percent_batch23\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch40.csv\n",
      "volume_weighted_all_percent_batch40\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch27.csv\n",
      "volume_weighted_all_percent_batch27\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch34.csv\n",
      "mean_path_length_all_percent_batch34\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch10.csv\n",
      "mean_path_length_all_percent_batch10\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch37.csv\n",
      "mean_path_length_all_percent_batch37\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch48.csv\n",
      "mean_path_length_all_percent_batch48\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch21.csv\n",
      "mean_path_length_all_percent_batch21\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch18.csv\n",
      "volume_weighted_all_percent_batch18\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch11.csv\n",
      "volume_weighted_all_percent_batch11\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch13.csv\n",
      "count_all_percent_batch13\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch16.csv\n",
      "mean_path_length_all_percent_batch16\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch36.csv\n",
      "count_all_percent_batch36\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch10.csv\n",
      "volume_weighted_all_percent_batch10\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch43.csv\n",
      "volume_weighted_all_percent_batch43\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch22.csv\n",
      "count_all_percent_batch22\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch45.csv\n",
      "volume_weighted_all_percent_batch45\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch48.csv\n",
      "count_all_percent_batch48\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch31.csv\n",
      "volume_weighted_all_percent_batch31\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch32.csv\n",
      "count_all_percent_batch32\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch42.csv\n",
      "volume_weighted_all_percent_batch42\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch14.csv\n",
      "volume_weighted_all_percent_batch14\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch47.csv\n",
      "count_all_percent_batch47\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch19.csv\n",
      "volume_weighted_all_percent_batch19\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch10.csv\n",
      "count_all_percent_batch10\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch39.csv\n",
      "mean_path_length_all_percent_batch39\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch28.csv\n",
      "volume_weighted_all_percent_batch28\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch30.csv\n",
      "count_all_percent_batch30\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch16.csv\n",
      "count_all_percent_batch16\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch20.csv\n",
      "mean_path_length_all_percent_batch20\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch28.csv\n",
      "mean_path_length_all_percent_batch28\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch27.csv\n",
      "mean_path_length_all_percent_batch27\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch26.csv\n",
      "mean_path_length_all_percent_batch26\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch17.csv\n",
      "mean_path_length_all_percent_batch17\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch25.csv\n",
      "mean_path_length_all_percent_batch25\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch23.csv\n",
      "volume_weighted_all_percent_batch23\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch37.csv\n",
      "volume_weighted_all_percent_batch37\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch29.csv\n",
      "count_all_percent_batch29\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch38.csv\n",
      "volume_weighted_all_percent_batch38\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/volume_weighted_all_percent_batch12.csv\n",
      "volume_weighted_all_percent_batch12\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch40.csv\n",
      "mean_path_length_all_percent_batch40\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch46.csv\n",
      "mean_path_length_all_percent_batch46\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/count_all_percent_batch38.csv\n",
      "count_all_percent_batch38\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/mean_path_length_all_percent_batch42.csv\n",
      "mean_path_length_all_percent_batch42\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "\n",
    "file_list = Path(main_path).glob('*batch??.csv')\n",
    "for file in file_list:\n",
    "    print(file)\n",
    "    df = pd.read_csv(file)\n",
    "    print(file.stem)\n",
    "    # melt data for boxplot\n",
    "    dd=pd.melt(df,id_vars=['Recon'],value_vars=['1','2','3','4','5'],var_name='Walk Length')\n",
    "    # seaborn boxplot with hue based on recon method\n",
    "    sns.boxplot(x='Walk Length',y='value',data=dd,hue='Recon')\n",
    "    # plt.show()\n",
    "    plt.ylabel('Pearson Score')\n",
    "    # save figure\n",
    "    plt.savefig(main_path+'/plots/'+file.stem+'_box_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix _stats.csv files to have correct summary statistics for each method sorted !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch0_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch1_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch2_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch3_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch4_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch5_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch6_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch7_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch8_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch9_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch10_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch11_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch12_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch13_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch14_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch15_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch16_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch17_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch18_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch19_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch20_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch21_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch22_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch23_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch24_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch25_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch26_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch27_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch28_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch29_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch30_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch31_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch32_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch33_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch34_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch35_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch36_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch37_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch38_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch39_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch40_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch41_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch42_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch43_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch44_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch45_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch46_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch47_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch48_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch49_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch0_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch1_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch2_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch3_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch4_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch5_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch6_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch7_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch8_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch9_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch10_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch11_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch12_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch13_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch14_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch15_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch16_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch17_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch18_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch19_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch20_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch21_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch22_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch23_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch24_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch25_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch26_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch27_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch28_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch29_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch30_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch31_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch32_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch33_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch34_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch35_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch36_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch37_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch38_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch39_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch40_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch41_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch42_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch43_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch44_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch45_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch46_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch47_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch48_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch49_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch0_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch1_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch2_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch3_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch4_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch5_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch6_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch7_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch8_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch9_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch10_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch11_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch12_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch13_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch14_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch15_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch16_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch17_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch18_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch19_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch20_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch21_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch22_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch23_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch24_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch25_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch26_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch27_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch28_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch29_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch30_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch31_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch32_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch33_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch34_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch35_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch36_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch37_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch38_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch39_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch40_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch41_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch42_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch43_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch44_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch45_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch46_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch47_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch48_walk_length_4_stats.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch49_walk_length_4_stats.csv\n"
     ]
    }
   ],
   "source": [
    "!for i in `seq 0 49`; do ii=\"/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch${i}_walk_length_4_stats.csv\"; echo ${ii}; head -n 1 ${ii} > tmp; tail -n 3 ${ii} | head -n 1 >> tmp; head -n 6 ${ii} | tail -n 1 >> tmp; head -n 4 ${ii} | tail -n 1 >> tmp; mv tmp ${ii::-4}_clean.csv; done\n",
    "!for i in `seq 0 49`; do ii=\"/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/mean_path_length_all_percent_batch${i}_walk_length_4_stats.csv\"; echo ${ii}; head -n 1 ${ii} > tmp; tail -n 3 ${ii} | head -n 1 >> tmp; head -n 6 ${ii} | tail -n 1 >> tmp; head -n 4 ${ii} | tail -n 1 >> tmp; mv tmp ${ii::-4}_clean.csv; done\n",
    "!for i in `seq 0 49`; do ii=\"/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/volume_weighted_all_percent_batch${i}_walk_length_4_stats.csv\"; echo ${ii}; head -n 1 ${ii} > tmp; tail -n 3 ${ii} | head -n 1 >> tmp; head -n 6 ${ii} | tail -n 1 >> tmp; head -n 4 ${ii} | tail -n 1 >> tmp; mv tmp ${ii::-4}_clean.csv; done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# get mean pearson scores and standard deviations for pearson scores for each recon method from _walk_length_4_stats.csv files\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats'\n",
    "for weighting in ['count', 'volume_weighted', 'mean_path_length']:\n",
    "    batch_list = []\n",
    "    file_list = Path(main_path).glob(str(weighting)+'_all_percent_batch*_walk_length_4_stats_clean.csv')\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file)\n",
    "        batch_list.append(df)\n",
    "    df = pd.concat(batch_list, axis=0)\n",
    "    # save dataframe to csv\n",
    "    df.to_csv(main_path+'/'+str(weighting)+'_walk_length_4_stats_50batches.csv', index=False)\n",
    "    # get mean and range for Mean and Stdev for each recon method from df, save to summary csv\n",
    "    recons = df['Recon'].unique()\n",
    "    for recon in recons:\n",
    "        df_recon = df[df['Recon'] == recon]\n",
    "        mean_mean_recon = df_recon['Mean'].mean()\n",
    "        # range_mean_recon = df_recon['Mean'].max() - df_recon['Mean'].min()\n",
    "        # std_mean_recon = df_recon['Mean'].std()\n",
    "        # mean_std_recon = df_recon['Stdev'].mean()\n",
    "        # calculate mean of standard deviations: Average S.D. = ((s12 +  s22 +  + sk2) / k)\n",
    "        mean_std_recon = math.sqrt(\n",
    "            (df_recon['Stdev']**2).sum() / len(df_recon['Stdev']))\n",
    "        # range_std_recon = df_recon['Stdev'].max() - df_recon['Stdev'].min()\n",
    "        # std_std_recon = df_recon['Stdev'].std()\n",
    "        df_recon_summary = pd.DataFrame(\n",
    "            {'Recon': [recon], 'Mean of Means': [mean_mean_recon], 'Mean of Standard Deviations': [mean_std_recon]}) # 'Range of Mean': [range_mean_recon],  'Standard Deviation of Means': [std_mean_recon],  'Range of Standard Deviations': [range_std_recon], 'Standard deviation of Standard Deviations': [std_std_recon]\n",
    "        df_recon_summary.to_csv(\n",
    "            main_path+'/'+str(weighting)+'_walk_length_4_stats_50batches_summary.csv', index=False, mode='a', header=False)\n",
    "\n",
    "# # combine all summary csvs into one csv\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats'\n",
    "# for weighting in ['count', 'volume_weighted', 'mean_path_length']:\n",
    "#     df_list = []\n",
    "#     file_list = Path(main_path).glob(str(weighting)+'_walk_length_4_stats_50batches_summary.csv')\n",
    "#     for file in file_list:\n",
    "#         df = pd.read_csv(file)\n",
    "#         df_list.append(df)\n",
    "#     df = pd.concat(df_list, axis=0)\n",
    "#     # save dataframe to csv\n",
    "#     df.to_csv(main_path+'/'+str(weighting)+'_walk_length_4_stats_50batches_summary_all.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to read in all _stats.csv files FROM WALK LENGTH 4\n",
    "#     and combine them into one dataframe\n",
    "def combine_stats_csv(main_path,weighting_method):\n",
    "    file_list = Path(main_path).glob(str(weighting_method)+'_*_walk_length_4*_stats.csv')\n",
    "    df = pd.DataFrame()\n",
    "    for file in file_list:\n",
    "        df = pd.concat([df, pd.read_csv(file)], axis=0)\n",
    "    return df\n",
    "\n",
    "# define a function to take the mean of the Pearson Scores for each recon method\n",
    "#     and return a dataframe with the mean Pearson Scores and standard deviations\n",
    "\n",
    "def get_mean_stdev(df):\n",
    "    df_mean_stdev = pd.DataFrame()\n",
    "    for recon in df['Recon'].unique():\n",
    "        df_mean = pd.concat([df_mean, pd.DataFrame({'Recon': recon, 'Mean': df.loc[df['Recon'] == recon]['Mean'],\n",
    "                                                    'Standard Deviation': df.loc[df['Recon'] == recon]['Pearson Score'].std()}, index=[0])], axis=0)\n",
    "    return df_mean_stdev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing Number of Significant Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of significant paired differences Streamline Count: DTI vs MSMT\n",
      "0.36\n",
      "Mean Test Statistic, Mean p-value, Mean Effect Size for Streamline Count: DTI vs MSMT\n",
      "(-1.716862891463359, 0.2820768212181645, -0.36854612480245574, 85.5, 0.4580886363983154, 0.3701298701298701)\n",
      "Fraction of significant paired differences Volume Weighted Streamline Count: DTI vs MSMT\n",
      "0.9\n",
      "Mean Test Statistic, Mean p-value, Mean Effect Size for Volume Weighted Streamline Count: DTI vs MSMT\n",
      "(-6.109079351107941, 0.010810647001937974, -1.3113904040656958, 18.0, 0.00057442982991535, 0.07792207792207789)\n",
      "Fraction of significant paired differences Mean Length: MSMT vs DTI\n",
      "0.98\n",
      "Mean Test Statistic, Mean p-value, Mean Effect Size for Mean Length: MSMT vs DTI\n",
      "(-14.104268544875435, 0.0010585575498094915, -3.0276579109683373, 1.2105263157894737, 1.9826387104235198e-06, 0.005240373661426279)\n",
      "Fraction of significant paired differences Streamline Count: GQI vs MSMT\n",
      "0.38\n",
      "Mean Test Statistic, Mean p-value, Mean Effect Size for Streamline Count: GQI vs MSMT\n",
      "(-1.117835567579394, 0.40159266639893887, -0.23995740641036023, 85.48, 0.33486494064331057, 0.37004329004329)\n",
      "Fraction of significant paired differences Volume Weighted Streamline Count: GQI vs MSMT\n",
      "0.98\n",
      "Mean Test Statistic, Mean p-value, Mean Effect Size for Volume Weighted Streamline Count: GQI vs MSMT\n",
      "(-5.446586757273576, 7.038681314588213e-05, -1.1691780705229309, 20.76923076923077, 0.0009594147021953423, 0.08991008991008989)\n",
      "Fraction of significant paired differences Mean Length: MSMT vs GQI\n",
      "0.82\n",
      "Mean Test Statistic, Mean p-value, Mean Effect Size for Mean Length: MSMT vs GQI\n",
      "(-5.60685446664589, 0.0045901738352187435, -1.2035815418273796, 31.818181818181817, 0.010740431872281115, 0.13774104683195587)\n",
      "Fraction of significant paired differences Streamline Count: DTI vs GQI\n",
      "0.14\n",
      "Mean Test Statistic, Mean p-value, Mean Effect Size for Streamline Count: DTI vs GQI\n",
      "(-0.6160021182102549, 0.3219377738431992, -0.1322325706177911, 119.27777777777777, 0.4913039207458496, 0.5163540163540163)\n",
      "Fraction of significant paired differences Volume Weighted Streamline Count: DTI vs GQI\n",
      "0.36\n",
      "Mean Test Statistic, Mean p-value, Mean Effect Size for Volume Weighted Streamline Count: DTI vs GQI\n",
      "(-2.17546414373026, 0.22115844081214064, -0.4669906279674464, 98.9047619047619, 0.37934244246709914, 0.42815914244485687)\n",
      "Fraction of significant paired differences Mean Length: GQI vs DTI\n",
      "0.8\n",
      "Mean Test Statistic, Mean p-value, Mean Effect Size for Mean Length: GQI vs DTI\n",
      "(-6.933181090420581, 0.039434163521876626, -1.4882941649756667, 1.6666666666666667, 2.002716064453125e-06, 0.0072150072150071924)\n"
     ]
    }
   ],
   "source": [
    "# read in all stats_all files from walk length 4\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats'\n",
    "file_list = Path(main_path).glob('*walk_length_4_stats_all.csv')\n",
    "df_list_count = []\n",
    "df_list_vol = []\n",
    "df_list_mean_length = []\n",
    "for file in file_list:\n",
    "    if 'count' in file.stem:\n",
    "        df_list_count.append(pd.read_csv(file))\n",
    "    elif 'vol' in file.stem:\n",
    "        df_list_vol.append(pd.read_csv(file))\n",
    "    elif 'mean_path_length' in file.stem:\n",
    "        df_list_mean_length.append(pd.read_csv(file))\n",
    "df_count = pd.concat(df_list_count)\n",
    "df_vol = pd.concat(df_list_vol)\n",
    "df_mean_length = pd.concat(df_list_mean_length)\n",
    "\n",
    "# define a function to calculate the Fraction of significant results for each Recon 1 vs Recon 2 method comparison\n",
    "# where the value of 'Recon 1' contains recon1 and of 'Recon 2' contains recon2\n",
    "def sig_frac(df, recon1, recon2):\n",
    "    df = df[(df['Recon 1'].str.contains(recon1)) & (df['Recon 2'].str.contains(recon2))]\n",
    "    sig_frac = df['significant_result'].sum() / (len(df['significant_result']))\n",
    "    return sig_frac\n",
    "\n",
    "def mean_stats(df, recon1, recon2):\n",
    "    df = df[(df['Recon 1'].str.contains(recon1)) & (df['Recon 2'].str.contains(recon2))]\n",
    "    test_statistic_list_ptt = []\n",
    "    p_value_list_ptt = []\n",
    "    effect_size_list_ptt = []\n",
    "    test_statistic_list_wsr = []\n",
    "    p_value_list_wsr = []\n",
    "    effect_size_list_wsr = []\n",
    "    # report mean t-statistic, p-value, effect size if shapiro-wilk test is significant\n",
    "    for index, row in df.iterrows():\n",
    "        if row['p-value Shapiro-Wilk test'] < 0.05:\n",
    "            test_statistic_list_wsr.append(row['Test statistic Wilcoxon Signed Rank Test'])\n",
    "            p_value_list_wsr.append(row['p-value Wilcoxon Signed Rank Test'])\n",
    "            effect_size_list_wsr.append(row['Effect Size r'])\n",
    "        elif row['p-value Shapiro-Wilk test'] > 0.05:\n",
    "            test_statistic_list_ptt.append(row['T-statistic Paired T-test'])\n",
    "            p_value_list_ptt.append(row['p-value Paired T-test'])\n",
    "            effect_size_list_ptt.append(row['Effect Size d'])\n",
    "    mean_test_statistic_ptt = np.mean(np.array(test_statistic_list_ptt))\n",
    "    mean_p_value_ptt = np.mean(np.array(p_value_list_ptt))\n",
    "    mean_effect_size_ptt = np.mean(np.array(effect_size_list_ptt))\n",
    "    mean_test_statistic_wsr = np.mean(np.array(test_statistic_list_wsr))\n",
    "    mean_p_value_wsr = np.mean(np.array(p_value_list_wsr))\n",
    "    mean_effect_size_wsr = np.mean(np.array(effect_size_list_wsr))\n",
    "    return mean_test_statistic_ptt, mean_p_value_ptt, mean_effect_size_ptt, mean_test_statistic_wsr, mean_p_value_wsr, mean_effect_size_wsr\n",
    "\n",
    "# Calculate the Fraction of significant results for each Recon 1 vs Recon 2 method comparison\n",
    "# for count where the value of 'Recon 1' contains DTI and of 'Recon 2' contains MSMT\n",
    "sig_frac_count_dm = sig_frac(df_count, 'DTI', 'MSMT')\n",
    "print(\"Fraction of significant paired differences Streamline Count: DTI vs MSMT\")\n",
    "print(sig_frac_count_dm)\n",
    "print(\"Mean Test Statistic, Mean p-value, Mean Effect Size for Streamline Count: DTI vs MSMT\")\n",
    "print(mean_stats(df_count, 'DTI', 'MSMT'))\n",
    "# for vol where the value of 'Recon 1' contains DTI and of 'Recon 2' contains MSMT\n",
    "sig_frac_vol_dm = sig_frac(df_vol, 'DTI', 'MSMT')\n",
    "print(\"Fraction of significant paired differences Volume Weighted Streamline Count: DTI vs MSMT\")\n",
    "print(sig_frac_vol_dm)\n",
    "print(\"Mean Test Statistic, Mean p-value, Mean Effect Size for Volume Weighted Streamline Count: DTI vs MSMT\")\n",
    "print(mean_stats(df_vol, 'DTI', 'MSMT'))\n",
    "# for mean length where the value of 'Recon 1' contains DTI and of 'Recon 2' contains MSMT\n",
    "sig_frac_mean_length_dm = sig_frac(df_mean_length, 'MSMT', 'DTI')\n",
    "print(\"Fraction of significant paired differences Mean Length: MSMT vs DTI\")\n",
    "print(sig_frac_mean_length_dm)\n",
    "print(\"Mean Test Statistic, Mean p-value, Mean Effect Size for Mean Length: MSMT vs DTI\")\n",
    "print(mean_stats(df_mean_length, 'MSMT', 'DTI'))\n",
    "sig_frac_count_gm = sig_frac(df_count, 'GQI', 'MSMT')\n",
    "print(\"Fraction of significant paired differences Streamline Count: GQI vs MSMT\")\n",
    "print(sig_frac_count_gm)\n",
    "print(\"Mean Test Statistic, Mean p-value, Mean Effect Size for Streamline Count: GQI vs MSMT\")\n",
    "print(mean_stats(df_count, 'GQI', 'MSMT'))\n",
    "sig_frac_vol_gm = sig_frac(df_vol, 'GQI', 'MSMT')\n",
    "print(\"Fraction of significant paired differences Volume Weighted Streamline Count: GQI vs MSMT\")\n",
    "print(sig_frac_vol_gm)\n",
    "print(\"Mean Test Statistic, Mean p-value, Mean Effect Size for Volume Weighted Streamline Count: GQI vs MSMT\")\n",
    "print(mean_stats(df_vol, 'GQI', 'MSMT'))\n",
    "sig_frac_mean_length_gm = sig_frac(df_mean_length, 'MSMT', 'GQI')\n",
    "print(\"Fraction of significant paired differences Mean Length: MSMT vs GQI\")\n",
    "print(sig_frac_mean_length_gm)\n",
    "print(\"Mean Test Statistic, Mean p-value, Mean Effect Size for Mean Length: MSMT vs GQI\")\n",
    "print(mean_stats(df_mean_length, 'MSMT', 'GQI'))\n",
    "sig_frac_count_dg = sig_frac(df_count, 'DTI', 'GQI')\n",
    "print(\"Fraction of significant paired differences Streamline Count: DTI vs GQI\")\n",
    "print(sig_frac_count_dg)\n",
    "print(\"Mean Test Statistic, Mean p-value, Mean Effect Size for Streamline Count: DTI vs GQI\")\n",
    "print(mean_stats(df_count, 'DTI', 'GQI'))\n",
    "sig_frac_vol_dg = sig_frac(df_vol, 'DTI', 'GQI')\n",
    "print(\"Fraction of significant paired differences Volume Weighted Streamline Count: DTI vs GQI\")\n",
    "print(sig_frac_vol_dg)\n",
    "print(\"Mean Test Statistic, Mean p-value, Mean Effect Size for Volume Weighted Streamline Count: DTI vs GQI\")\n",
    "print(mean_stats(df_vol, 'DTI', 'GQI'))\n",
    "sig_frac_mean_length_dg = sig_frac(df_mean_length, 'GQI', 'DTI')\n",
    "print(\"Fraction of significant paired differences Mean Length: GQI vs DTI\")\n",
    "print(sig_frac_mean_length_dg)\n",
    "print(\"Mean Test Statistic, Mean p-value, Mean Effect Size for Mean Length: GQI vs DTI\")\n",
    "print(mean_stats(df_mean_length, 'GQI', 'DTI'))\n",
    "\n",
    "\n",
    "# ### Other tail\n",
    "\n",
    "# # Calculate the Fraction of significant results for each Recon 1 vs Recon 2 method comparison\n",
    "# # for count where the value of 'Recon 1' contains DTI and of 'Recon 2' contains MSMT\n",
    "# sig_frac_count_dm = sig_frac(df_count, 'MSMT', 'DTI')\n",
    "# print(\"Fraction of significant paired differences Streamline Count: MSMT vs DTI\")\n",
    "# print(sig_frac_count_dm)\n",
    "# # for vol where the value of 'Recon 1' contains DTI and of 'Recon 2' contains MSMT\n",
    "# sig_frac_vol_dm = sig_frac(df_vol, 'MSMT', 'DTI')\n",
    "# print(\"Fraction of significant paired differences Volume Weighted Streamline Count: MSMT vs DTI\")\n",
    "# print(sig_frac_vol_dm)\n",
    "# # for mean length where the value of 'Recon 1' contains DTI and of 'Recon 2' contains MSMT\n",
    "# sig_frac_mean_length_dm = sig_frac(df_mean_length, 'DTI', 'MSMT')\n",
    "# print(\"Fraction of significant paired differences Mean Length: MSMT vs DTI\")\n",
    "# print(sig_frac_mean_length_dm)\n",
    "# sig_frac_count_gm = sig_frac(df_count, 'MSMT', 'GQI')\n",
    "# print(\"Fraction of significant paired differences Streamline Count: MSMT vs GQI\")\n",
    "# print(sig_frac_count_gm)\n",
    "# sig_frac_vol_gm = sig_frac(df_vol, 'MSMT', 'GQI')\n",
    "# print(\"Fraction of significant paired differences Volume Weighted Streamline Count: MSMT vs GQI\")\n",
    "# print(sig_frac_vol_gm)\n",
    "# sig_frac_mean_length_gm = sig_frac(df_mean_length, 'GQI', 'MSMT')\n",
    "# print(\"Fraction of significant paired differences Mean Length: GQI vs MSMT\")\n",
    "# print(sig_frac_mean_length_gm)\n",
    "# sig_frac_count_dg = sig_frac(df_count, 'GQI', 'DTI')\n",
    "# print(\"Fraction of significant paired differences Streamline Count: GQI vs DTI\")\n",
    "# print(sig_frac_count_dg)\n",
    "# sig_frac_vol_dg = sig_frac(df_vol, 'GQI', 'DTI')\n",
    "# print(\"Fraction of significant paired differences Volume Weighted Streamline Count: GQI vs DTI\")\n",
    "# print(sig_frac_vol_dg)\n",
    "# sig_frac_mean_length_dg = sig_frac(df_mean_length, 'DTI', 'GQI')\n",
    "# print(\"Fraction of significant paired differences Mean Length: DTI vs GQI\")\n",
    "# print(sig_frac_mean_length_dg)\n",
    "\n",
    "# Fraction of significant paired differences Streamline Count: MSMT vs DTI\n",
    "# 0.1\n",
    "# Fraction of significant paired differences Volume Weighted Streamline Count: MSMT vs DTI\n",
    "# 0.0\n",
    "# Fraction of significant paired differences Mean Length: MSMT vs DTI\n",
    "# 0.0\n",
    "# Fraction of significant paired differences Streamline Count: MSMT vs GQI\n",
    "# 0.08\n",
    "# Fraction of significant paired differences Volume Weighted Streamline Count: MSMT vs GQI\n",
    "# 0.0\n",
    "# Fraction of significant paired differences Mean Length: GQI vs MSMT\n",
    "# 0.0\n",
    "# Fraction of significant paired differences Streamline Count: GQI vs DTI\n",
    "# 0.2\n",
    "# Fraction of significant paired differences Volume Weighted Streamline Count: GQI vs DTI\n",
    "# 0.08\n",
    "# Fraction of significant paired differences Mean Length: DTI vs GQI\n",
    "# 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21575/2977612436.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_21575/2977612436.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_21575/2977612436.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_21575/2977612436.py:28: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_21575/2977612436.py:37: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_21575/2977612436.py:38: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_21575/2977612436.py:45: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_vol_ttest = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n"
     ]
    }
   ],
   "source": [
    "# read in z-score paired t-test results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('volume_*_walk_length_4_z_score_ttest_rel_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_paired_ttest_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_paired_ttest_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Node Volume Weighted Streamline Count', recon2='GQI Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_paired_ttest_results_min.csv')\n",
    "\n",
    "# read in z-score paired t-test results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "df_vol_ttest = pd.DataFrame()\n",
    "df_vol_ttest = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_vol_ttest.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_vol_ttest.to_csv(main_path+'/stats/walk_4_volume_weighted_paired_ttest_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21575/127232398.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_21575/127232398.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_21575/127232398.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_21575/127232398.py:28: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_21575/127232398.py:37: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_21575/127232398.py:38: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_21575/127232398.py:45: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_vol_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n"
     ]
    }
   ],
   "source": [
    "# read in z-score wilcoxon signed rank test results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('volume_*_walk_length_4_wilcoxon_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_volume_weighted_wilcoxon_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_volume_weighted_wilcoxon_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Node Volume Weighted Streamline Count', recon2='GQI Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_volume_weighted_wilcoxon_results_min.csv')\n",
    "\n",
    "# combine all volume weighted wilcoxon signed rank test results at walk length 4\n",
    "verbose = False\n",
    "df_vol_wil = pd.DataFrame()\n",
    "df_vol_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_vol_wil.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_vol_wil.to_csv(main_path+'/stats/walk_4_volume_weighted_wilcoxon_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21575/4074295172.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_21575/4074295172.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_21575/4074295172.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_21575/4074295172.py:28: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_21575/4074295172.py:37: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_21575/4074295172.py:38: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_21575/4074295172.py:46: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_count_ttest = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n"
     ]
    }
   ],
   "source": [
    "# read in z-score paired t-test results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('count_*_walk_length_4_z_score_ttest_rel_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_paired_ttest_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_paired_ttest_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Streamline Count', recon2='GQI Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_paired_ttest_results_min.csv')\n",
    "\n",
    "\n",
    "# combine all count t-test results at walk length 4\n",
    "verbose = False\n",
    "df_count_ttest = pd.DataFrame()\n",
    "df_count_ttest = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_count_ttest.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_count_ttest.to_csv(main_path+'/stats/walk_4_count_paired_ttest_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21575/1331690065.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_21575/1331690065.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_21575/1331690065.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_21575/1331690065.py:28: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_21575/1331690065.py:37: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_21575/1331690065.py:38: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_21575/1331690065.py:45: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_count_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n"
     ]
    }
   ],
   "source": [
    "# read in z-score wilcoxon signed rank test results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('count_*_walk_length_4_wilcoxon_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_count_wilcoxon_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_count_wilcoxon_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Streamline Count', recon2='GQI Streamline Count', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_count_wilcoxon_results_min.csv')\n",
    "\n",
    "# combine all volume weighted wilcoxon signed rank test results at walk length 4\n",
    "verbose = False\n",
    "df_count_wil = pd.DataFrame()\n",
    "df_count_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_count_wil.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_count_wil.to_csv(main_path+'/stats/walk_4_count_wilcoxon_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21575/248381940.py:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:24: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:25: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:32: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:33: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:65: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_count_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:66: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_count_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:73: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_count_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:74: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_count_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:81: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_count_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_21575/248381940.py:82: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_count_shapiro_wilk_results_std.csv')\n"
     ]
    }
   ],
   "source": [
    "# read in shapiro-wilk results from all batches in main_path/stats with walk length 4\n",
    "verbose = False\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/'\n",
    "\n",
    "shapiro_file_list = Path(main_path).glob('volume_*_walk_length_4_shapiro_wilk_results.csv')\n",
    "df_sh = pd.DataFrame()\n",
    "for sh_file in shapiro_file_list:\n",
    "    if verbose:\n",
    "        print(sh_file)\n",
    "    df_sh = pd.concat([df_sh,pd.read_csv(sh_file)],axis=0)\n",
    "df_sh_dm = get_df_int(df=df_sh, recon1='DTI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_gm = get_df_int(df=df_sh, recon1='GQI Node Volume Weighted Streamline Count', recon2='MSMT CSD SIFT2 Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_gm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_mean.csv')\n",
    "df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_dq = get_df_int(df=df_sh, recon1='DTI Node Volume Weighted Streamline Count', recon2='GQI Node Volume Weighted Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dq)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_std.csv')\n",
    "\n",
    "# read in mean and std files for shapiro-wilk at walk 4, summarize results for different recon1 reecon2 combinations\n",
    "verbose = False\n",
    "shapiro_list = ['walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_msmt_volume_weighted_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_GQI_volume_weighted_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_GQI_msmt_volume_weighted_shapiro_wilk_results_std.csv']\n",
    "df_summary = pd.DataFrame()\n",
    "for s_file in shapiro_list:\n",
    "    if verbose:\n",
    "        print(s_file)\n",
    "    df = pd.read_csv(main_path+'/'+s_file, index_col=0)\n",
    "    df_summary = pd.concat([df_summary,df],axis=1)\n",
    "\n",
    "df_summary.columns = ['DTI MSMT mean', 'DTI MSMT std', 'DTI GQI mean', 'DTI GQI std', 'GQI MSMT mean', 'GQI MSMT std']\n",
    "df_summary.to_csv(main_path+'/walk_4_volume_weighted_shapiro_wilk_summary.csv')\n",
    "\n",
    "\n",
    "shapiro_file_list = Path(main_path).glob('count_*_walk_length_4_shapiro_wilk_results.csv')\n",
    "df_sh = pd.DataFrame()\n",
    "for sh_file in shapiro_file_list:\n",
    "    if verbose:\n",
    "        print(sh_file)\n",
    "    df_sh = pd.concat([df_sh,pd.read_csv(sh_file)],axis=0)\n",
    "df_sh_dm = get_df_int(df=df_sh, recon1='DTI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_count_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_count_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_gm = get_df_int(df=df_sh, recon1='GQI Streamline Count', recon2='MSMT CSD SIFT2 Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_gm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_count_shapiro_wilk_results_mean.csv')\n",
    "df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_count_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_dq = get_df_int(df=df_sh, recon1='DTI Streamline Count', recon2='GQI Streamline Count', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dq)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_count_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_count_shapiro_wilk_results_std.csv')\n",
    "\n",
    "# read in mean and std files for shapiro-wilk at walk 4, summarize results for different recon1 reecon2 combinations\n",
    "verbose = False\n",
    "shapiro_list = ['walk_4_DTI_msmt_count_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_msmt_count_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_DTI_GQI_count_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_GQI_count_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_GQI_msmt_count_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_GQI_msmt_count_shapiro_wilk_results_std.csv']\n",
    "df_summary = pd.DataFrame()\n",
    "for s_file in shapiro_list:\n",
    "    if verbose:\n",
    "        print(s_file)\n",
    "    df = pd.read_csv(main_path+'/'+s_file, index_col=0)\n",
    "    df_summary = pd.concat([df_summary,df],axis=1)\n",
    "\n",
    "df_summary.columns = ['DTI MSMT mean', 'DTI MSMT std', 'DTI GQI mean', 'DTI GQI std', 'GQI MSMT mean', 'GQI MSMT std']\n",
    "df_summary.to_csv(main_path+'/walk_4_count_shapiro_wilk_summary.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch41_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch11_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch29_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch4_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch12_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch2_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch48_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch28_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch27_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch33_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch18_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch26_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch21_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch43_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch34_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch47_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch37_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch22_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch32_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch36_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch23_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch40_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch49_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch0_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch15_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch31_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch14_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch44_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch8_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch42_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch7_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch39_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch19_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch1_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch46_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch16_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch17_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch35_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch13_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch45_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch3_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch24_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch38_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch20_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch30_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch10_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch6_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch25_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch9_walk_length_4_z_score_ttest_rel_results.csv\n",
      "/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats/count_all_percent_batch5_walk_length_4_z_score_ttest_rel_results.csv\n",
      "mean\n",
      "T-statistic Paired T-test   -0.670309\n",
      "p-value Paired T-test        0.423778\n",
      "Effect Size d               -0.143890\n",
      "Mean 1                       0.618146\n",
      "Mean 2                       0.623587\n",
      "dtype: float64\n",
      "std\n",
      "T-statistic Paired T-test    2.414981\n",
      "p-value Paired T-test        0.365590\n",
      "Effect Size d                0.518406\n",
      "Mean 1                       0.017933\n",
      "Mean 2                       0.019918\n",
      "dtype: float64\n",
      "max\n",
      "Recon 1                                 GQI Streamline Count\n",
      "Recon 2                      MSMT CSD SIFT2 Streamline Count\n",
      "T-statistic Paired T-test                           4.655343\n",
      "p-value Paired T-test                               0.999924\n",
      "Effect Size d                                       0.999328\n",
      "Mean 1                                              0.663792\n",
      "Mean 2                                              0.654287\n",
      "dtype: object\n",
      "min\n",
      "Recon 1                                 GQI Streamline Count\n",
      "Recon 2                      MSMT CSD SIFT2 Streamline Count\n",
      "T-statistic Paired T-test                          -5.210666\n",
      "p-value Paired T-test                               0.000021\n",
      "Effect Size d                                      -1.118535\n",
      "Mean 1                                              0.577404\n",
      "Mean 2                                              0.559967\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21575/547111287.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  print(df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].mean())\n",
      "/tmp/ipykernel_21575/547111287.py:13: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  print(df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].std())\n",
      "/tmp/ipykernel_21575/547111287.py:20: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].mean().to_csv(main_path+'/stats/walk_4_gqi_msmt_count_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_21575/547111287.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].std().to_csv(main_path+'/stats/walk_4_gqi_msmt_count_paired_ttest_results_std.csv')\n"
     ]
    }
   ],
   "source": [
    "# read in z-score paired t-test results from all batches in main_path/stats with walk length 4\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('count_*_walk_length_4_z_score_ttest_rel_results.csv')\n",
    "df = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    print(file)\n",
    "    df = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is GQI Streamline Count and Recon 2 is MSMT CSD SIFT2 Streamline Count\n",
    "print('mean')\n",
    "print(df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].mean())\n",
    "print('std')\n",
    "print(df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].std())\n",
    "print('max')\n",
    "print(df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].max())\n",
    "print('min')\n",
    "print(df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].min())\n",
    "\n",
    "# save out mean, std, max, min to csv\n",
    "df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].mean().to_csv(main_path+'/stats/walk_4_gqi_msmt_count_paired_ttest_results_mean.csv')\n",
    "df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].std().to_csv(main_path+'/stats/walk_4_gqi_msmt_count_paired_ttest_results_std.csv')\n",
    "df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].max().to_csv(main_path+'/stats/walk_4_gqi_msmt_count_paired_ttest_results_max.csv')\n",
    "df[(df['Recon 1']=='GQI Streamline Count') & (df['Recon 2']=='MSMT CSD SIFT2 Streamline Count')].min().to_csv(main_path+'/stats/walk_4_gqi_msmt_count_paired_ttest_results_min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21575/2967870238.py:15: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:25: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:26: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:35: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_paired_ttest_results_mean.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:36: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_paired_ttest_results_std.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:43: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_ml_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
      "/tmp/ipykernel_21575/2967870238.py:61: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:62: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:71: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:72: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:81: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_wilcoxon_results_mean.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:82: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_wilcoxon_results_std.csv')\n",
      "/tmp/ipykernel_21575/2967870238.py:89: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_ml_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n"
     ]
    }
   ],
   "source": [
    "# read in paired t-test results from all batches in main_path/stats with walk length 4\n",
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/'\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('mean_path_length_*_walk_length_4_z_score_ttest_rel_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Mean Length', recon2='MSMT CSD Mean Length', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_paired_ttest_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_paired_ttest_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_paired_ttest_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_paired_ttest_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Mean Length', recon2='MSMT CSD Mean Length', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_paired_ttest_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_paired_ttest_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_paired_ttest_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_paired_ttest_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Mean Length', recon2='GQI Mean Length', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_paired_ttest_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_paired_ttest_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_paired_ttest_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_paired_ttest_results_min.csv')\n",
    "\n",
    "# combine all volume weighted wilcoxon signed rank test results at walk length 4\n",
    "verbose = False\n",
    "df_ml_wil = pd.DataFrame()\n",
    "df_ml_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_ml_wil.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_ml_wil.to_csv(main_path+'/stats/walk_4_mean_length_paired_ttest_results.csv')\n",
    "\n",
    "\n",
    "\n",
    "# main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v3/scrambled_dataset/'\n",
    "file_list = Path(main_path+'/stats').glob('mean_path_length_*_walk_length_4_wilcoxon_results.csv')\n",
    "df_ii = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    if verbose:\n",
    "        print(file)\n",
    "    df_ii = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "df_dm = get_df_int(df=df_ii, recon1='DTI Mean Length', recon2='MSMT CSD Mean Length', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dm.mean().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_wilcoxon_results_mean.csv')\n",
    "df_dm.std().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_wilcoxon_results_std.csv')\n",
    "df_dm.max().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_wilcoxon_results_max.csv')\n",
    "df_dm.min().to_csv(main_path+'/stats/walk_4_DTI_msmt_mean_length_wilcoxon_results_min.csv')\n",
    "\n",
    "df_gm = get_df_int(df=df_ii, recon1='GQI Mean Length', recon2='MSMT CSD Mean Length', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_gm)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_gm.mean().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_wilcoxon_results_mean.csv')\n",
    "df_gm.std().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_wilcoxon_results_std.csv')\n",
    "df_gm.max().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_wilcoxon_results_max.csv')\n",
    "df_gm.min().to_csv(main_path+'/stats/walk_4_GQI_msmt_mean_length_wilcoxon_results_min.csv')\n",
    "\n",
    "df_dq = get_df_int(df=df_ii, recon1='DTI Mean Length', recon2='GQI Mean Length', i=None)\n",
    "# get mean and standard deviation, max, min of t-statistic and p-value for the row where Recon 1 is DTI Node Volume Weighted Streamline Count and Recon 2 is MSMT CSD SIFT2 Node Volume Weighted Streamline Count\n",
    "if verbose:\n",
    "    print_stats(df=df_dq)\n",
    "# save out mean, std, max, min to one csv\n",
    "df_dq.mean().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_wilcoxon_results_mean.csv')\n",
    "df_dq.std().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_wilcoxon_results_std.csv')\n",
    "df_dq.max().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_wilcoxon_results_max.csv')\n",
    "df_dq.min().to_csv(main_path+'/stats/walk_4_DTI_GQI_mean_length_wilcoxon_results_min.csv')\n",
    "\n",
    "# combine all volume weighted wilcoxon signed rank test results at walk length 4\n",
    "verbose = False\n",
    "df_ml_wil = pd.DataFrame()\n",
    "df_ml_wil = pd.concat([df_dm.mean(),df_dm.std(),df_gm.mean(),df_gm.std(),df_dq.mean(),df_dq.std()],axis=1)\n",
    "df_ml_wil.columns = ['DTI_MSMT_mean','DTI_MSMT_std','GQI_MSMT_mean','GQI_MSMT_std','DTI_GQI_mean','DTI_GQI_std']\n",
    "df_ml_wil.to_csv(main_path+'/stats/walk_4_mean_length_wilcoxon_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21575/4287466499.py:14: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_mean_length_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_21575/4287466499.py:15: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_mean_length_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_21575/4287466499.py:22: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_mean_length_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_21575/4287466499.py:23: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_mean_length_shapiro_wilk_results_std.csv')\n",
      "/tmp/ipykernel_21575/4287466499.py:30: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_mean_length_shapiro_wilk_results_mean.csv')\n",
      "/tmp/ipykernel_21575/4287466499.py:31: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_mean_length_shapiro_wilk_results_std.csv')\n"
     ]
    }
   ],
   "source": [
    "main_path = '/home/paul/thesis/dev/SAY_sf_prediction_v4/dataset/stats'\n",
    "shapiro_file_list = Path(main_path).glob('mean_path_length_*_walk_length_4_shapiro_wilk_results.csv')\n",
    "df_sh = pd.DataFrame()\n",
    "for sh_file in shapiro_file_list:\n",
    "    if verbose:\n",
    "        print(sh_file)\n",
    "    df_sh = pd.concat([df_sh,pd.read_csv(sh_file)],axis=0)\n",
    "\n",
    "df_sh_dm = get_df_int(df=df_sh, recon1='DTI Mean Length', recon2='MSMT CSD Mean Length', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dm.mean().to_csv(main_path+'/walk_4_DTI_msmt_mean_length_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dm.std().to_csv(main_path+'/walk_4_DTI_msmt_mean_length_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_gm = get_df_int(df=df_sh, recon1='GQI Mean Length', recon2='MSMT CSD Mean Length', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_gm)\n",
    "# save out mean, std to one csv\n",
    "df_sh_gm.mean().to_csv(main_path+'/walk_4_GQI_msmt_mean_length_shapiro_wilk_results_mean.csv')\n",
    "df_sh_gm.std().to_csv(main_path+'/walk_4_GQI_msmt_mean_length_shapiro_wilk_results_std.csv')\n",
    "\n",
    "df_sh_dq = get_df_int(df=df_sh, recon1='DTI Mean Length', recon2='GQI Mean Length', i=None)\n",
    "# get mean and standard deviation of W statistic and p-value\n",
    "if verbose:\n",
    "    print_stats(df=df_sh_dq)\n",
    "# save out mean, std to one csv\n",
    "df_sh_dq.mean().to_csv(main_path+'/walk_4_DTI_GQI_mean_length_shapiro_wilk_results_mean.csv')\n",
    "df_sh_dq.std().to_csv(main_path+'/walk_4_DTI_GQI_mean_length_shapiro_wilk_results_std.csv')\n",
    "\n",
    "# read in mean and std files for shapiro-wilk at walk 4, summarize results for different recon1 reecon2 combinations\n",
    "verbose = False\n",
    "shapiro_list = ['walk_4_DTI_msmt_mean_length_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_msmt_mean_length_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_DTI_GQI_mean_length_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_DTI_GQI_mean_length_shapiro_wilk_results_std.csv',\n",
    "              'walk_4_GQI_msmt_mean_length_shapiro_wilk_results_mean.csv',\n",
    "              'walk_4_GQI_msmt_mean_length_shapiro_wilk_results_std.csv']\n",
    "df_summary = pd.DataFrame()\n",
    "for s_file in shapiro_list:\n",
    "    if verbose:\n",
    "        print(s_file)\n",
    "    df = pd.read_csv(main_path+'/'+s_file, index_col=0)\n",
    "    df_summary = pd.concat([df_summary,df],axis=1)\n",
    "\n",
    "df_summary.columns = ['DTI MSMT mean', 'DTI MSMT std', 'DTI GQI mean', 'DTI GQI std', 'GQI MSMT mean', 'GQI MSMT std']\n",
    "df_summary.to_csv(main_path+'/walk_4_mean_length_shapiro_wilk_summary.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
